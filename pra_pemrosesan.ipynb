{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PraPemrosesan Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "with open('kampus_merdeka.json') as content:\n",
    "  data1 = json.load(content)\n",
    "\n",
    "# Mendapatkan semua data ke dalam list\n",
    "tags = [] # data tag\n",
    "inputs = [] # data input atau pattern\n",
    "responses = {} # data respon\n",
    "words = [] # Data kata\n",
    "classes = [] # Data Kelas atau Tag\n",
    "documents = [] # Data Kalimat Dokumen\n",
    "ignore_words = ['?', '!'] # Mengabaikan tanda spesial karakter\n",
    "# Tambahkan data intents dalam json\n",
    "for intent in data1['intents']:\n",
    "  responses[intent['tag']]=intent['responses']\n",
    "  for lines in intent['patterns']:\n",
    "    inputs.append(lines)\n",
    "    tags.append(intent['tag'])\n",
    "    # digunakan untuk pattern atau teks pertanyaan dalam json\n",
    "    for pattern in intent['patterns']:\n",
    "      w = nltk.word_tokenize(pattern)\n",
    "      words.extend(w)\n",
    "      documents.append((w, intent['tag']))\n",
    "      # tambahkan ke dalam list kelas dalam data\n",
    "      if intent['tag'] not in classes:\n",
    "        classes.append(intent['tag'])\n",
    "\n",
    "# Konversi data json ke dalam dataframe\n",
    "data = pd.DataFrame({\"patterns\":inputs, \"tags\":tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patterns</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hallo</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hai</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>halo</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hei</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hi</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Apa itu PMMB?</td>\n",
       "      <td>penjelasan_PMMB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Apa saja persyaratan IISMA?</td>\n",
       "      <td>persyaratan_IISMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Bagaimana periode pelaksanaan dan cara mendaft...</td>\n",
       "      <td>periode_dan_pendaftaran_PMMB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Apa saja manfaat apabila mengikuti PMMB?</td>\n",
       "      <td>manfaat_PMMB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Apa saja universitas yang tergabung dalam prog...</td>\n",
       "      <td>mitra_PMMB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             patterns  \\\n",
       "0                                               hallo   \n",
       "1                                                 hai   \n",
       "2                                                halo   \n",
       "3                                                 hei   \n",
       "4                                                  hi   \n",
       "..                                                ...   \n",
       "70                                      Apa itu PMMB?   \n",
       "71                        Apa saja persyaratan IISMA?   \n",
       "72  Bagaimana periode pelaksanaan dan cara mendaft...   \n",
       "73           Apa saja manfaat apabila mengikuti PMMB?   \n",
       "74  Apa saja universitas yang tergabung dalam prog...   \n",
       "\n",
       "                            tags  \n",
       "0                       greeting  \n",
       "1                       greeting  \n",
       "2                       greeting  \n",
       "3                       greeting  \n",
       "4                       greeting  \n",
       "..                           ...  \n",
       "70               penjelasan_PMMB  \n",
       "71             persyaratan_IISMA  \n",
       "72  periode_dan_pendaftaran_PMMB  \n",
       "73                  manfaat_PMMB  \n",
       "74                    mitra_PMMB  \n",
       "\n",
       "[75 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Punctuaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Punctuations (Menghilangkan Punktuasi) \n",
    "data['patterns'] = data['patterns'].apply(lambda wrd:[ltrs.lower() for ltrs in wrd if ltrs not in string.punctuation])\n",
    "data['patterns'] = data['patterns'].apply(lambda wrd: ''.join(wrd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patterns</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hallo</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hai</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>halo</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hei</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hi</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>apa itu pmmb</td>\n",
       "      <td>penjelasan_PMMB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>apa saja persyaratan iisma</td>\n",
       "      <td>persyaratan_IISMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>bagaimana periode pelaksanaan dan cara mendaft...</td>\n",
       "      <td>periode_dan_pendaftaran_PMMB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>apa saja manfaat apabila mengikuti pmmb</td>\n",
       "      <td>manfaat_PMMB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>apa saja universitas yang tergabung dalam prog...</td>\n",
       "      <td>mitra_PMMB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             patterns  \\\n",
       "0                                               hallo   \n",
       "1                                                 hai   \n",
       "2                                                halo   \n",
       "3                                                 hei   \n",
       "4                                                  hi   \n",
       "..                                                ...   \n",
       "70                                       apa itu pmmb   \n",
       "71                         apa saja persyaratan iisma   \n",
       "72  bagaimana periode pelaksanaan dan cara mendaft...   \n",
       "73            apa saja manfaat apabila mengikuti pmmb   \n",
       "74  apa saja universitas yang tergabung dalam prog...   \n",
       "\n",
       "                            tags  \n",
       "0                       greeting  \n",
       "1                       greeting  \n",
       "2                       greeting  \n",
       "3                       greeting  \n",
       "4                       greeting  \n",
       "..                           ...  \n",
       "70               penjelasan_PMMB  \n",
       "71             persyaratan_IISMA  \n",
       "72  periode_dan_pendaftaran_PMMB  \n",
       "73                  manfaat_PMMB  \n",
       "74                    mitra_PMMB  \n",
       "\n",
       "[75 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"clean_data_chatbot.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 kata-kata yang di lematisasi :  ['afternoon', 'apa', 'apabila', 'bagaimana', 'bai', 'banyak', 'bersertifikat', 'bro', 'bumn', 'bye', 'byee', 'cara', 'dadah', 'dah', 'dalam', 'dan', 'good', 'hai', 'hallo', 'halo', 'hei', 'hi', 'hy', 'iisma', 'independen', 'itu', 'jumpa', 'kampus', 'kasih', 'kawan', 'kemendikbud', 'kemensos', 'magang', 'mahasiswa', 'makasih', 'malam', 'manfaat', 'mendaftar', 'mengajar', 'mengikuti', 'merdeka', 'mitra', 'morning', 'muda', 'pagi', 'pejuang', 'pelaksanaan', 'periode', 'persyaratan', 'pertukaran', 'pmmb', 'program', 'saja', 'sampai', 'see', 'selamat', 'si', 'siang', 'sore', 'studi', 'tergabung', 'terima', 'thank', 'thanks', 'tinggal', 'tujuan', 'universitas', 'yang', 'you']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "print (len(words), \"kata-kata yang di lematisasi : \", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menyortir Data Kelas Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 classes ['goodbye', 'greeting', 'manfaat_IISMA', 'manfaat_PMMB', 'manfaat_kampus_mengajar', 'manfaat_magang', 'manfaat_pejuang_muda', 'manfaat_pertukaran_mahasiswa', 'manfaat_studi_independen', 'mitra_IISMA', 'mitra_PMMB', 'mitra_magang', 'mitra_studi_independen', 'penjelasan_IISMA', 'penjelasan_PMMB', 'penjelasan_kampus_mengajar', 'penjelasan_kampus_merdeka', 'penjelasan_magang', 'penjelasan_pejuang_muda', 'penjelasan_pertukaran_mahasiswa', 'penjelasan_studi_independen', 'periode_dan_pendaftaran_IISMA', 'periode_dan_pendaftaran_PMMB', 'periode_dan_pendaftaran_kampus_mengajar', 'periode_dan_pendaftaran_magang', 'periode_dan_pendaftaran_pejuang_muda', 'periode_dan_pendaftaran_pertukaran_mahasiswa', 'periode_dan_pendaftaran_studi_independen', 'persyaratan_IISMA', 'persyaratan_kampus_mengajar', 'persyaratan_kampus_merdeka', 'persyaratan_magang', 'persyaratan_pejuang_muda', 'persyaratan_pertukaran_mahasiswa', 'persyaratan_studi_independen', 'program_kampus_merdeka', 'terimakasih', 'tujuan_kampus_merdeka']\n"
     ]
    }
   ],
   "source": [
    "# sorting pada data class\n",
    "classes = sorted(list(set(classes)))\n",
    "print (len(classes), \"classes\", classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mencari Jumlah Keseluruhan Data Teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425 documents\n"
     ]
    }
   ],
   "source": [
    "# documents = kombinasi antara data pattern dengan data tag dalam intents json\n",
    "print (len(documents), \"documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[37],\n",
       " [38],\n",
       " [39],\n",
       " [40],\n",
       " [41],\n",
       " [42],\n",
       " [43],\n",
       " [44],\n",
       " [45],\n",
       " [46],\n",
       " [47],\n",
       " [48],\n",
       " [49],\n",
       " [50],\n",
       " [51],\n",
       " [52],\n",
       " [53],\n",
       " [31],\n",
       " [54],\n",
       " [55, 31],\n",
       " [56, 57],\n",
       " [58, 59],\n",
       " [60],\n",
       " [61, 32],\n",
       " [62],\n",
       " [63, 32],\n",
       " [33, 34],\n",
       " [64],\n",
       " [33, 34, 65],\n",
       " [1, 6, 4, 5],\n",
       " [1, 66, 4, 5],\n",
       " [1, 7, 8, 4, 5],\n",
       " [1, 2, 3, 4, 5],\n",
       " [3, 4, 5, 67],\n",
       " [3, 4, 5, 68],\n",
       " [3, 4, 5, 69],\n",
       " [18, 19],\n",
       " [20],\n",
       " [4, 21],\n",
       " [9],\n",
       " [22, 23, 5],\n",
       " [24, 25],\n",
       " [26],\n",
       " [1, 6, 18, 19],\n",
       " [1, 2, 7, 18, 19],\n",
       " [10, 11, 12, 13, 14, 15, 3, 18, 19],\n",
       " [1, 2, 16, 17, 8, 18, 19],\n",
       " [1, 2, 35, 28, 29, 30, 18, 19],\n",
       " [1, 6, 20, 27],\n",
       " [1, 2, 7, 20, 27],\n",
       " [10, 11, 12, 13, 14, 15, 3, 20, 27],\n",
       " [1, 2, 16, 17, 8, 20, 27],\n",
       " [1, 2, 35, 28, 29, 30, 20, 27],\n",
       " [1, 6, 4, 21],\n",
       " [1, 2, 7, 4, 21],\n",
       " [10, 11, 12, 13, 14, 15, 3, 4, 21],\n",
       " [1, 2, 16, 17, 8, 4, 21],\n",
       " [1, 6, 9],\n",
       " [1, 2, 7, 9],\n",
       " [10, 11, 12, 13, 14, 15, 3, 9],\n",
       " [1, 2, 16, 17, 8, 9],\n",
       " [1, 2, 36, 28, 29, 30, 3, 9],\n",
       " [1, 6, 22, 23, 5],\n",
       " [1, 2, 7, 22, 23, 5],\n",
       " [10, 11, 12, 13, 14, 15, 3, 22, 23, 5],\n",
       " [1, 2, 16, 17, 8, 22, 23, 5],\n",
       " [1, 6, 24, 25],\n",
       " [1, 2, 7, 24, 25],\n",
       " [10, 11, 12, 13, 14, 15, 3, 24, 25],\n",
       " [1, 2, 16, 17, 8, 24, 25],\n",
       " [1, 6, 26],\n",
       " [1, 2, 7, 9],\n",
       " [10, 11, 12, 13, 14, 15, 3, 26],\n",
       " [1, 2, 16, 17, 8, 26],\n",
       " [1, 2, 36, 28, 29, 30, 3, 26]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the data (Tokenisasi Data)\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(data['patterns'])\n",
    "train = tokenizer.texts_to_sequences(data['patterns'])\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0 37]\n",
      " [ 0  0  0  0  0  0  0  0  0 38]\n",
      " [ 0  0  0  0  0  0  0  0  0 39]\n",
      " [ 0  0  0  0  0  0  0  0  0 40]\n",
      " [ 0  0  0  0  0  0  0  0  0 41]\n",
      " [ 0  0  0  0  0  0  0  0  0 42]\n",
      " [ 0  0  0  0  0  0  0  0  0 43]\n",
      " [ 0  0  0  0  0  0  0  0  0 44]\n",
      " [ 0  0  0  0  0  0  0  0  0 45]\n",
      " [ 0  0  0  0  0  0  0  0  0 46]\n",
      " [ 0  0  0  0  0  0  0  0  0 47]\n",
      " [ 0  0  0  0  0  0  0  0  0 48]\n",
      " [ 0  0  0  0  0  0  0  0  0 49]\n",
      " [ 0  0  0  0  0  0  0  0  0 50]\n",
      " [ 0  0  0  0  0  0  0  0  0 51]\n",
      " [ 0  0  0  0  0  0  0  0  0 52]\n",
      " [ 0  0  0  0  0  0  0  0  0 53]\n",
      " [ 0  0  0  0  0  0  0  0  0 31]\n",
      " [ 0  0  0  0  0  0  0  0  0 54]\n",
      " [ 0  0  0  0  0  0  0  0 55 31]\n",
      " [ 0  0  0  0  0  0  0  0 56 57]\n",
      " [ 0  0  0  0  0  0  0  0 58 59]\n",
      " [ 0  0  0  0  0  0  0  0  0 60]\n",
      " [ 0  0  0  0  0  0  0  0 61 32]\n",
      " [ 0  0  0  0  0  0  0  0  0 62]\n",
      " [ 0  0  0  0  0  0  0  0 63 32]\n",
      " [ 0  0  0  0  0  0  0  0 33 34]\n",
      " [ 0  0  0  0  0  0  0  0  0 64]\n",
      " [ 0  0  0  0  0  0  0 33 34 65]\n",
      " [ 0  0  0  0  0  0  1  6  4  5]\n",
      " [ 0  0  0  0  0  0  1 66  4  5]\n",
      " [ 0  0  0  0  0  1  7  8  4  5]\n",
      " [ 0  0  0  0  0  1  2  3  4  5]\n",
      " [ 0  0  0  0  0  0  3  4  5 67]\n",
      " [ 0  0  0  0  0  0  3  4  5 68]\n",
      " [ 0  0  0  0  0  0  3  4  5 69]\n",
      " [ 0  0  0  0  0  0  0  0 18 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  4 21]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0 22 23  5]\n",
      " [ 0  0  0  0  0  0  0  0 24 25]\n",
      " [ 0  0  0  0  0  0  0  0  0 26]\n",
      " [ 0  0  0  0  0  0  1  6 18 19]\n",
      " [ 0  0  0  0  0  1  2  7 18 19]\n",
      " [ 0 10 11 12 13 14 15  3 18 19]\n",
      " [ 0  0  0  1  2 16 17  8 18 19]\n",
      " [ 0  0  1  2 35 28 29 30 18 19]\n",
      " [ 0  0  0  0  0  0  1  6 20 27]\n",
      " [ 0  0  0  0  0  1  2  7 20 27]\n",
      " [ 0 10 11 12 13 14 15  3 20 27]\n",
      " [ 0  0  0  1  2 16 17  8 20 27]\n",
      " [ 0  0  1  2 35 28 29 30 20 27]\n",
      " [ 0  0  0  0  0  0  1  6  4 21]\n",
      " [ 0  0  0  0  0  1  2  7  4 21]\n",
      " [ 0 10 11 12 13 14 15  3  4 21]\n",
      " [ 0  0  0  1  2 16 17  8  4 21]\n",
      " [ 0  0  0  0  0  0  0  1  6  9]\n",
      " [ 0  0  0  0  0  0  1  2  7  9]\n",
      " [ 0  0 10 11 12 13 14 15  3  9]\n",
      " [ 0  0  0  0  1  2 16 17  8  9]\n",
      " [ 0  0  1  2 36 28 29 30  3  9]\n",
      " [ 0  0  0  0  0  1  6 22 23  5]\n",
      " [ 0  0  0  0  1  2  7 22 23  5]\n",
      " [10 11 12 13 14 15  3 22 23  5]\n",
      " [ 0  0  1  2 16 17  8 22 23  5]\n",
      " [ 0  0  0  0  0  0  1  6 24 25]\n",
      " [ 0  0  0  0  0  1  2  7 24 25]\n",
      " [ 0 10 11 12 13 14 15  3 24 25]\n",
      " [ 0  0  0  1  2 16 17  8 24 25]\n",
      " [ 0  0  0  0  0  0  0  1  6 26]\n",
      " [ 0  0  0  0  0  0  1  2  7  9]\n",
      " [ 0  0 10 11 12 13 14 15  3 26]\n",
      " [ 0  0  0  0  1  2 16 17  8 26]\n",
      " [ 0  0  1  2 36 28 29 30  3 26]]\n"
     ]
    }
   ],
   "source": [
    "# Melakukan proses padding pada data\n",
    "x_train = pad_sequences(train)\n",
    "# Menampilkan hasil padding\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  0  0  0  0  0  0  0  0  0\n",
      " 36 36 36 36 36 16 37 30 35 35 35 35 35 35 35 35 35 35 35 20 34 27  8 12\n",
      " 17 31 24  5 11 15 29 23  4 13 28 21  2  9 19 33 26  7 18 32 25  6 14 28\n",
      " 22  3 10]\n"
     ]
    }
   ],
   "source": [
    "# Melakukan konversi data label tags dengan encoding\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(data['tags'])\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Length, Output Length and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# Melihat hasil input pada data teks\n",
    "input_shape = x_train.shape[1]\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumlah kata unik :  69\n",
      "panjang output:  38\n"
     ]
    }
   ],
   "source": [
    "# Melakukan definisi tiap kalimat dan kata pada data teks\n",
    "vocabulary = len(tokenizer.word_index)\n",
    "print(\"jumlah kata unik : \", vocabulary)\n",
    "\n",
    "# Melakukan pemeriksaan pada data output label teks\n",
    "output_length = le.classes_.shape[0]\n",
    "print(\"panjang output: \", output_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input length dan output length terlihat sangat jelas hasilnya. Mereka adalah untuk bentuk input dan bentuk output dari data train atau latih yang akan diproses pada algoritma LSTM yang akan dilatih.\n",
    "\n",
    "Vocabulary Size adalah untuk lapisan penyematan untuk membuat representasi vektor unik untuk setiap kata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model Words & Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan hasil pemrosesan teks dengan menggunakan pickle\n",
    "pickle.dump(words, open('words.pkl','wb'))\n",
    "pickle.dump(classes, open('classes.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Label Encoder & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(le, open('le.pkl','wb'))\n",
    "pickle.dump(tokenizer, open('tokenizers.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
